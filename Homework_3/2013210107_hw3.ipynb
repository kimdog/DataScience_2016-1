{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from quandl import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias = 1\n",
    "\n",
    "X = np.array([\n",
    "        [bias, 0,0],\n",
    "        [bias, 0,1],\n",
    "        [bias, 1,0],\n",
    "        [bias, 1,1]\n",
    "    ])\n",
    "\n",
    "Y_AND = np.array([\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1]\n",
    "    ])\n",
    "\n",
    "Y_OR = np.array([\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "\n",
    "\"\"\"\n",
    "The given weight is only appliable for sigmoid function.\n",
    "\n",
    "TO DO :\n",
    "\n",
    "Fill in the weights w1, w2, w3 for each theta so that the result gives the right answer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#your code here\n",
    "and_w1 = -1.5\n",
    "and_w2 = 1\n",
    "and_w3 = 1\n",
    "\n",
    "theta_AND = np.array([\n",
    "        [and_w1, and_w2, and_w3]\n",
    "    ])\n",
    "\n",
    "or_w1 = -0.5\n",
    "or_w2 = 1\n",
    "or_w3 = 1\n",
    "\n",
    "theta_OR = np.array([\n",
    "        [or_w1, or_w2, or_w3]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.5  1.   1. ]]\n",
      "[[-0.5  1.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "print theta_AND\n",
    "print theta_OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "\n",
    "def sigmoid(g, d=False):\n",
    "        if(d):\n",
    "            gradientSigmoid = g * (1-g) # your code here. The differentiation of sigmoid by z\n",
    "            return gradientSigmoid\n",
    "        else:\n",
    "            normalSigmoid = 1/(1+np.exp(-g))                           # your code here. The normal sigmoid\n",
    "            return normalSigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND\n",
      "[[ 0.  0.  0.  1.]]\n",
      "\n",
      "OR\n",
      "[[ 0.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "flag = False # gradients are only needed in the case for back propagation.\n",
    "\n",
    "\"\"\"\n",
    "Check if AND = [0, 0, 0, 1], OR = [0, 1, 1, 1]\n",
    "\"\"\"\n",
    "\n",
    "h_AND_sigmoid = np.round(sigmoid(np.dot(X, theta_AND.T), flag))\n",
    "print \"AND\"\n",
    "print h_AND_sigmoid.T\n",
    "\n",
    "print \"\"\n",
    "\n",
    "h_OR_sigmoid = np.round(sigmoid(np.dot(X, theta_OR.T), flag))\n",
    "print \"OR\"\n",
    "print h_OR_sigmoid.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 3\n",
    "\n",
    "\"\"\"\n",
    "This will create a Neural Network with specified inputs and outputs.\n",
    "\n",
    "Input : When building a constructor, you should pass on the size of the input layer, the output layer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, InputLayer, OutputLayer):\n",
    "        # Define the number of nodes in each layers\n",
    "        self.inputLayer = InputLayer\n",
    "        self.hiddenLayer = InputLayer\n",
    "        self.output = OutputLayer\n",
    "\n",
    "        # create node weight matrices\n",
    "        # components' range is -1 <= x <= 1\n",
    "        self.theta0 = 2*np.random.random((self.hiddenLayer, self.inputLayer+1))-1\n",
    "        self.theta1 = 2*np.random.random((self.output, self.hiddenLayer+1))-1\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, g, d=False):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        if(d):\n",
    "            gradientSigmoid = 1/(1+np.exp(-g)) * (1-(1/(1+np.exp(-g)))) # your code here. Same code above.\n",
    "            return gradientSigmoid\n",
    "        else:\n",
    "            normalSigmoid = 1/(1+np.exp(-g))                            # your code here. Same code above.\n",
    "            return normalSigmoid\n",
    "\n",
    "        \n",
    "    def costFunction(self, X, y):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        m = len(y) # 'm' represents the number of training examples.\n",
    "        result_X = self.forward_propagation(X)\n",
    "        \n",
    "        temp_cost = 0\n",
    "        for i in range(m):\n",
    "            temp_cost += - ( y[i]*np.log(result_X[i]) + (1-y[i])*np.log(1-result_X[i]) )\n",
    "        \n",
    "        cost = temp_cost / m  # write your code here. Code for the cost of 'logistic regression'\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        \n",
    "        # Forward Propagate Inputs Through Network\n",
    "        m = len(X) # your code here. 'm' means the total number of rows(training examples) in your input.\n",
    "        \n",
    "        # Before starting your propagation, you need to add bias in your input unit\n",
    "        bias = np.tile(1,(m,1))\n",
    "        \n",
    "        # your code here. Add bias nodes to the 2 dimensional array representing your input.\n",
    "        self.a1 = np.concatenate((bias, X), axis=1)\n",
    "        \n",
    "        # Input Layer -> Hidden Layer\n",
    "        for i in range(len(self.theta0)):\n",
    "            tempA = np.dot(self.a1, self.theta0[i])\n",
    "            tempA = np.array([tempA]).T\n",
    "            if(i == 0) :\n",
    "                Hidden = tempA\n",
    "            else :\n",
    "                Hidden = np.concatenate((Hidden,tempA), axis=1)\n",
    "        \n",
    "        self.z2 = Hidden # your code here. The dot product of a1 and theta0\n",
    "        self.a2 = self.sigmoid(self.z2, False) # your code here. The sigmoid activated result of z2\n",
    "        \n",
    "        # Before starting next propagation, add bias 1 in your hidden layer unit\n",
    "        self.a2 = np.concatenate((bias, self.a2), axis=1) # your code here. Stack the hidden nodes with bias '1'\n",
    "        \n",
    "        # Hidden Layer -> Output Layer\n",
    "        for i in range(len(self.theta1)):\n",
    "            tempB = np.dot(self.a2, self.theta1[i])\n",
    "            tempB = np.array([tempB]).T\n",
    "            if(i == 0) :\n",
    "                Out = tempB\n",
    "            else :\n",
    "                Out = np.concatenate((Out,tempA), axis=1)\n",
    "        \n",
    "        self.z3 = Out # your code here. The dot product of a2 and theta1\n",
    "        self.ho = self.sigmoid(self.z3, False) # your code here. The sigmoid activated result of z3\n",
    "        \n",
    "        # Final output of the neural network\n",
    "        return self.ho\n",
    "\n",
    "    def backward_propagation(self, X, y, alpha):\n",
    "        # for e in range(epoch):\n",
    "        max_iteration = 500\n",
    "        self.err = np.ones(max_iteration)\n",
    "        final_error = 1\n",
    "        iteration = 0\n",
    "        \n",
    "        while self.err[iteration] > 1e-10 :\n",
    "            # Forward propogate\n",
    "            a3 = self.forward_propagation(X)\n",
    "            \n",
    "            # Backward Propagate\n",
    "            delta3 = a3 - y\n",
    "            theta1_grad = np.dot(delta3.T, self.a2)\n",
    "            delta2 = np.dot(delta3, self.theta1[:, 1:]) * self.sigmoid(self.z2, d=True)\n",
    "            theta0_grad = np.dot(delta2.T, self.a1)\n",
    "\n",
    "            change = theta0_grad\n",
    "            self.theta0 -= (alpha-change*0.1) * theta0_grad\n",
    "            change = theta1_grad\n",
    "            self.theta1 -= (alpha-change*0.1) * theta1_grad\n",
    "            \n",
    "            if(iteration + 1 == 500):\n",
    "                return self.err\n",
    "            \n",
    "            final_error = self.costFunction(X, y)\n",
    "            self.err[iteration+1] = final_error\n",
    "            iteration = iteration+1\n",
    "            \n",
    "        return self.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR training data\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before training =  [ 0.6966106]\n",
      "Error after training =  [ 0.01413625]\n",
      "[[ 0.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, 1)\n",
    "\n",
    "print \"Error before training = \", nn.costFunction(X, y)\n",
    "\n",
    "# learning thetas\n",
    "max_iteration = 500\n",
    "min_err = 1\n",
    "for i in range(10):\n",
    "    nn = NeuralNetwork(2, 1)\n",
    "    \n",
    "    cost = np.zeros(max_iteration)\n",
    "    cost = nn.backward_propagation(X, y, 0.5)         # learning rate = 0.5 , epoch = 1\n",
    "    err = nn.costFunction(X, y)\n",
    "    \n",
    "    if (min_err > err):\n",
    "        min_err = err\n",
    "        result = nn.forward_propagation(X)\n",
    "        min_cost = cost\n",
    "    \n",
    "print \"Error after training = \", min_err\n",
    "print np.round(result.T)\n",
    "\n",
    "# visualize cost: y-axis, iterations(epochs): x-axis\n",
    "plt.plot(np.arange(0., max_iteration, 1.0), min_cost.T)\n",
    "plt.axis([0, max_iteration, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Network in Finance\n",
    "#Problem 4\n",
    "\n",
    "import requests\n",
    "from pattern import web\n",
    "import re\n",
    "import requests\n",
    "from pattern import web\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_page(request_id):\n",
    "    print \"fetching \" , request_id\n",
    "        \n",
    "    target_url = 'http://finance.yahoo.com/q/hp?s=' + request_id + '&g=m'\n",
    "    r = requests.get(target_url) # Get requests for the given URL.\n",
    "    \n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  %5EGSPC\n"
     ]
    }
   ],
   "source": [
    "SP500_code = '%5EGSPC'\n",
    "SP500_html = scrap_page(SP500_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str2number(number_str):\n",
    "        if number_str is None:\n",
    "            return None\n",
    "        else:\n",
    "            number_str = re.sub(\"[^0-9]\", \"\", number_str) \n",
    "            # return int(number_str)\n",
    "            if(number_str == \"\") : \n",
    "                number_str = '1';\n",
    "            return int(float(number_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 5 . Parsing HTML\n",
    "\n",
    "def parsing(html):\n",
    "    element = web.Element(html)\n",
    "    \n",
    "    prices_text = element('.yfnc_datamodoutline1 table tr')\n",
    "    prices_text = prices_text[1:len(prices_text)-1]  # we top and bottom data is not needed\n",
    "            \n",
    "    # Your code here. Notice that misusing 'tr' approach will lead to the wrong result.\n",
    "    # Check what is being retrieved in your system constantly.\n",
    "    index = prices_text\n",
    "    lst = [] # Construct a list to contain all the prices you need.\n",
    "    \n",
    "    for price in index:\n",
    "        sampleText = price.source\n",
    "        #remove not needed column\n",
    "        chk = sampleText.find('Dividend')\n",
    "        if(chk == -1):\n",
    "            #print sampleText,\"\\n\"\n",
    "            cp = re.split('</td><td class=\"yfnc_tabledata1\" align=\"right\">',sampleText)\n",
    "            data = cp.pop()\n",
    "            num_data = round(str2number(data) * 0.01, 2)\n",
    "            lst.append(num_data)\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500 = parsing(SP500_html)\n",
    "len(SP500) # This should be 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2047.21, 2065.3, 2059.74, 1932.23, 1940.24, 2043.94, 2080.41, 2079.36, 1920.03, 1972.18, 2103.84, 2063.11, 2107.39, 2085.51, 2067.89, 2104.5, 1994.99, 2058.9, 2067.56, 2018.05, 1972.29, 2003.37, 1930.67, 1960.23, 1923.57, 1883.95, 1872.34, 1859.45, 1782.59, 1848.36, 1805.81, 1756.54, 1681.55, 1632.97, 1685.73, 1606.28, 1630.74, 1597.57, 1569.19, 1514.68, 1498.11, 1426.19, 1416.18, 1412.16, 1440.67, 1406.58, 1379.32, 1362.16, 1310.33, 1397.91, 1408.47, 1365.68, 1312.41, 1257.6, 1246.96, 1253.3, 1131.42, 1218.89, 1292.28, 1320.64, 1345.2, 1363.61, 1325.83, 1327.22, 1286.12, 1257.64, 1180.55]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  FLOAT !! ERROR LATER     CHANGE CODE !!! INT --> FLOAT BLACK BOARD QUESTION\n",
    "'''\n",
    "print SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  %5EDJI\n"
     ]
    }
   ],
   "source": [
    "#Dow Jones Industrial Average\n",
    "DowJones_code = '%5EDJI'\n",
    "DowJones_html = scrap_page(DowJones_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DJI = parsing(DowJones_html)\n",
    "len(DJI) # This should be 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  PIR\n"
     ]
    }
   ],
   "source": [
    "#Prime Interest Rate\n",
    "PIR_code = 'PIR'\n",
    "PIR_html = scrap_page(PIR_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIR = parsing(PIR_html)\n",
    "len(PIR) # This should be 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  NDAQ\n"
     ]
    }
   ],
   "source": [
    "#NASDAQ Composite\n",
    "NASDAQ_code = 'NDAQ'\n",
    "NASDAQ_html = scrap_page(NASDAQ_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSDQ = parsing(NASDAQ_html)\n",
    "len(NSDQ) # this should be 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP500 :\n",
      "[ 1.16992665  1.22689819  1.20938786  0.8078157   0.8330419   1.15962831\n",
      "  1.27448469  1.27117789  0.76939377  0.93363177  1.3482737   1.22000113\n",
      "  1.35945385  1.29054632  1.23505497  1.35035226  1.00546819  1.20674242\n",
      "  1.23401569  1.07809194  0.9339782   1.03185965  0.80290273  0.89599718\n",
      "  0.78054242  0.65576563  0.61920181  0.57860683  0.33654867  0.54368067\n",
      "  0.40967631  0.25450839  0.01833949 -0.13465538  0.03150373 -0.21871123\n",
      " -0.1416784  -0.24614197 -0.3355202  -0.50719064 -0.55937518 -0.78587562\n",
      " -0.8174005  -0.83006084 -0.74027319 -0.84763415 -0.93348512 -0.98752777\n",
      " -1.15075799 -0.87493891 -0.8416819  -0.9764421  -1.14420737 -1.31682261\n",
      " -1.35033158 -1.33036477 -1.71420616 -1.43873351 -1.20760355 -1.11828831\n",
      " -1.04094055 -0.98296123 -1.10194324 -1.09756566 -1.22700348 -1.31669664\n",
      " -1.55947916]\n",
      "DJI :\n",
      "[ 1.14297743  1.25173159  1.21220856  0.69062511  0.66821906  1.0961345\n",
      "  1.22775444  1.20259003  0.58716449  0.69577136  1.21433758  1.18293787\n",
      "  1.35753101  1.28158252  1.2528385   1.41199289  0.9800515   1.27379396\n",
      "  1.27610152  1.08073145  0.92557623  0.95037018  0.71151362  0.82903382\n",
      "  0.7801913   0.71934235  0.66436272  0.60368338  0.32567871  0.71747667\n",
      "  0.49866058  0.25734472  0.07163335 -0.07090842  0.23671954 -0.02659175\n",
      "  0.06534002 -0.05774598 -0.17435565 -0.40825788 -0.49480684 -0.83243303\n",
      " -0.86749716 -0.83586089 -0.6838077  -0.83836929 -0.87504024 -0.93243454\n",
      " -1.14963936 -0.78356373 -0.7842734  -0.9003073  -1.04275981 -1.22814535\n",
      " -1.30486153 -1.34533079 -1.81024746 -1.49774551 -1.26131702 -1.1403154\n",
      " -1.07093252 -0.96347719 -1.18254323 -1.22422652 -1.37348564 -1.51382252\n",
      " -1.76889892]\n",
      "PIR :\n",
      "[-1.72323769 -1.4224354  -1.41220402 -1.83169022 -2.01790117 -1.8173663\n",
      " -1.48586989 -1.35490835 -1.4694997  -0.82287707 -0.50161204 -0.35223402\n",
      " -0.3379101  -0.34814148 -0.10258858 -0.47705675  0.44990544  0.16342706\n",
      " -0.14760661 -0.32358619 -0.53025987  0.21867646  0.08362237  0.13887177\n",
      "  0.56449679  0.68931951  0.79572577  0.80391086  0.8407438   1.58968013\n",
      "  1.43825584  1.16814766  0.90008575  1.35845115  1.66129972  1.64902208\n",
      "  1.5917264   1.59581895  1.5446621   1.44439467  1.29706293  0.96556652\n",
      "  0.81209596  1.04127866  0.71796735  0.66885677  0.29234233  0.27392586\n",
      "  0.24937057  0.41511878  0.5951909   0.40488741  0.09999256 -0.20490229\n",
      " -0.27038306 -0.4729642  -0.98862528 -0.81878453 -0.75944258 -0.65099005\n",
      " -0.583463   -0.5363987  -0.91905196 -0.9313296  -1.06433742 -0.85152491\n",
      " -0.99271783]\n",
      "NSDQ : \n",
      "[ 2.01921081  1.95762791  2.30835005  2.05826338  1.96138296  1.67524776\n",
      "  1.68951697  1.63469317  1.29523619  1.11724658  1.10598141  0.94151\n",
      "  1.13977691  0.90996754  1.08044704  1.01135403  0.67640311  0.84988666\n",
      "  0.61482021  0.49390744  0.43232453  0.4984135   0.40453713  0.14393631\n",
      "  0.08010037  0.0072523   0.01025634  0.10638575  0.0891125   0.20852325\n",
      "  0.16196057 -0.11666452 -0.35623703 -0.52821855 -0.34572288 -0.31718446\n",
      " -0.422326   -0.56426708 -0.36224512 -0.41706892 -0.65513941 -0.89320991\n",
      " -0.95704584 -0.9885883  -1.02388582 -1.06218738 -1.07420356 -1.07645659\n",
      " -1.13203141 -0.94202562 -0.84814925 -0.81735779 -0.92925843 -0.94653169\n",
      " -0.82411689 -0.90823013 -1.04341211 -1.00435954 -0.97732314 -0.89095687\n",
      " -0.87518564 -0.76253399 -0.85265531 -0.65739245 -0.94878472 -1.00135549\n",
      " -1.16132084]\n"
     ]
    }
   ],
   "source": [
    "#Nomalization\n",
    "\n",
    "SP500 = (SP500 - np.mean(SP500))/np.std(SP500)\n",
    "DJI = (DJI-np.mean(DJI))/np.std(DJI)\n",
    "PIR = (PIR-np.mean(PIR))/np.std(PIR)\n",
    "NSDQ = (NSDQ-np.mean(NSDQ))/np.std(NSDQ)\n",
    "\n",
    "print \"SP500 :\\n\", SP500\n",
    "print \"DJI :\\n\", DJI\n",
    "print \"PIR :\\n\", PIR\n",
    "print \"NSDQ : \\n\", NSDQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 6. Constructing an Input Matrix\n",
    "\n",
    "\n",
    "# your code here. Matrix for every indices you have retrieved. It should have the size of (67 x 4)\n",
    "# Notice that the last column will be seperated as your target value 'y'.\n",
    "\n",
    "mtrx = [[SP500[0], PIR[0], DJI[0], NSDQ[0]]]\n",
    "for i in range(len(SP500)-1):\n",
    "    tempMtrx = [[SP500[i+1], PIR[i+1], DJI[i+1], NSDQ[i+1]]]\n",
    "    mtrx = np.concatenate((mtrx, tempMtrx), axis=0)\n",
    "\n",
    "# So make sure that the 'NASDAQ' index comes in the last column of your matrix.\n",
    "# For example, [[S&P500 column], [PIR column], [Dow Jones column], [NASDAQ column]] will be a valid matrix\n",
    "\n",
    "\n",
    "mtrx = np.array(mtrx)\n",
    "# your code here. Convert the matrix into a numpy 2 dimensional array\n",
    "\n",
    "np.random.shuffle(mtrx) # Shuffle the matrix using 'shuffle' function in 'numpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem7 . constructing Data sets\n",
    "\n",
    "X = mtrx[:,np.array([True, True, True, False])]\n",
    "y = mtrx[:,np.array([False, False, False, True])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55L, 3L)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = X[12:len(X),:]\n",
    "train_X.shape # this should be (55L, 3L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55L, 1L)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = y[12:len(y),:]\n",
    "train_Y.shape # this should be (55L, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12L, 3L)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = X[0:12,:]\n",
    "test_X.shape # this should be (12L, 3L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12L, 1L)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = y[0:12,:]\n",
    "test_Y.shape # this should be (12L, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 8. Using LASSO to predict the NASDAQ index\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.05108692],\n",
       "       [ 1.22801116],\n",
       "       [-0.84516734],\n",
       "       [-0.67531038],\n",
       "       [-0.89487854],\n",
       "       [-0.23434498],\n",
       "       [-0.89354729],\n",
       "       [ 1.26196897],\n",
       "       [ 0.85219849],\n",
       "       [-0.75107115],\n",
       "       [ 1.1964596 ],\n",
       "       [-0.84030683]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Lasso(alpha = 0.1)# your code here. Follow the code above.\n",
    "clf.fit(train_X, train_Y)\n",
    "pred = clf.predict(test_X)\n",
    "\n",
    "tempList = []\n",
    "for i in pred:\n",
    "    tempList.append([i])\n",
    "    \n",
    "pred = np.array(tempList) # your code here. Make all your predictions into a list.\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04341211],\n",
       "       [ 1.63469317],\n",
       "       [-0.81735779],\n",
       "       [-0.94878472],\n",
       "       [-0.92925843],\n",
       "       [-0.35623703],\n",
       "       [-0.90823013],\n",
       "       [ 1.68951697],\n",
       "       [ 1.11724658],\n",
       "       [-0.41706892],\n",
       "       [ 2.30835005],\n",
       "       [-0.89320991]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y # Compare with the prediction you did just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.22455928],\n",
       "       [ 1.62612164],\n",
       "       [-0.97225313],\n",
       "       [-0.76277156],\n",
       "       [-1.01685164],\n",
       "       [-0.39212129],\n",
       "       [-1.05454427],\n",
       "       [ 1.79920201],\n",
       "       [ 0.66762843],\n",
       "       [-0.68435475],\n",
       "       [ 1.63990874],\n",
       "       [-0.93003353]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Problem9. Using Ridge to predict the NASDAQ index\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "max = 0\n",
    "\n",
    "for degree in range(5):\n",
    "    model = make_pipeline(PolynomialFeatures(3), Ridge())\n",
    "    model.fit(train_X, train_Y)\n",
    "    if(model.score(test_X, test_Y)) > max:\n",
    "        fin_model = model\n",
    "        max = model.score(test_X, test_Y)\n",
    "        \n",
    "Xf = fin_model.predict(test_X)\n",
    "Xf # Compare the result from the test_Y above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95119396348742369"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_model.score(test_X, test_Y) # Observe the results of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Homework3 , Support Vector Machine\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from glob import glob \n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# set your path to the unzipped blog folder.\n",
    "DATA_FILE_PATH_PATTERN = \"C:/Users/sec/Desktop/dataScience/Homework_3/blogs/*.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionaries, to be used in spliting the words and removing the stop words like \"a, the, ...\"\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop word set and the stemmer object,\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_user_info(file_path):\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    From the file_path,\n",
    "    \n",
    "    extract each records,\n",
    "    \n",
    "    and return dictionary,\n",
    "    \n",
    "    which contains each records,\n",
    "    \n",
    "    user_id, gender, age, fields, constellation\n",
    "    \n",
    "    (WARN: \"age\" column should be labelled with 1, 0,\n",
    "    \n",
    "    which represents the user is teenager or not.)\n",
    "    \"\"\"    \n",
    "    ####################################################\n",
    "    # split by \".\"\n",
    "    #7596.male.26.Internet.Scorpio\n",
    "    \n",
    "    cp = re.split('\\.|/',file_path)\n",
    "    last = len(cp)-2\n",
    "    \n",
    "    user_id = cp[last-4]\n",
    "    gender = cp[last-3]\n",
    "    #age = cp[last-2]\n",
    "    fields = cp[last-1]\n",
    "    constellation = cp[last]\n",
    "    \n",
    "    if(cp[last-2]<20) :\n",
    "        age = 1\n",
    "    else :\n",
    "        age = 0\n",
    "\n",
    "    dict = {\n",
    "     'user_id': user_id,\n",
    "     'gender': gender,\n",
    "     'age': age,\n",
    "     'fields': fields,\n",
    "     'constellation': constellation\n",
    "       }\n",
    "    \n",
    "    file_info_dict = dict\n",
    "    \n",
    "    return file_info_dict\n",
    "\n",
    "\n",
    "def parse_file(file_path):\n",
    "    # parse user_info from file_path string\n",
    "    user_info = parse_user_info(file_path)\n",
    "    \n",
    "    f = open(file_path, \"r\")\n",
    "    tmp_str = \"\"\n",
    "    for l in f:\n",
    "        tmp_str += l.strip()\n",
    "    f.close()\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    Get only post section from \"tmp_str\"\n",
    "    \n",
    "    Hint: use re package to parse the string.\n",
    "    \n",
    "    wr_tmp_str contains all of the post data.\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    cp_post = re.split('<post>|</post>',tmp_str)\n",
    "    wr_tmp_str = ''\n",
    "\n",
    "    #only odd lines are Post data\n",
    "    for i in range(1,len(cp_post)-1,2):\n",
    "        wr_tmp_str += cp_post[i] + ' '\n",
    "        \n",
    "    # merge post text\n",
    "    user_info['post'] = wr_tmp_str.decode('utf-8', errors = \"replace\")\n",
    "    \n",
    "    return user_info\n",
    "\n",
    "\n",
    "def load_data(file_path_pattern, ratio=1.):\n",
    "    file_paths = glob(file_path_pattern)\n",
    "    file_cnt = int(len(file_paths) * ratio)\n",
    "    \n",
    "    data = list()\n",
    "    cnt = 0\n",
    "    for file_path in file_paths[:file_cnt]:\n",
    "        user_data = parse_file(file_path)\n",
    "        data.append(user_data)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            print cnt,'/',file_cnt\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>constellation</th>\n",
       "      <th>fields</th>\n",
       "      <th>gender</th>\n",
       "      <th>post</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Leo</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Well, everyone got up and going this morning. ...</td>\n",
       "      <td>blogs\\1000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Yeah, sorry for not writing for a whole there,...</td>\n",
       "      <td>blogs\\1000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>Arts</td>\n",
       "      <td>male</td>\n",
       "      <td>cupid,please hear my cry, cupid, please let yo...</td>\n",
       "      <td>blogs\\1004904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Arts</td>\n",
       "      <td>female</td>\n",
       "      <td>and did i mention that i no longer have to dea...</td>\n",
       "      <td>blogs\\1005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>male</td>\n",
       "      <td>B-Logs: The Business Blogs Paradox    urlLink ...</td>\n",
       "      <td>blogs\\1005545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Religion</td>\n",
       "      <td>male</td>\n",
       "      <td>1/03 DrKioni.com Awarded ByRegion.net Healers ...</td>\n",
       "      <td>blogs\\1007188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Architecture</td>\n",
       "      <td>female</td>\n",
       "      <td>Friday    My dear wife was walking on her grad...</td>\n",
       "      <td>blogs\\100812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Sorry, but I gotta..I couldn't remember the wo...</td>\n",
       "      <td>blogs\\1008329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>Planning the Marathon   I checked Active.com, ...</td>\n",
       "      <td>blogs\\1009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Technology</td>\n",
       "      <td>female</td>\n",
       "      <td>The astute among you will note that this run i...</td>\n",
       "      <td>blogs\\1011153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>MSN conversation: 11.17am   Iggbalbollywall  (...</td>\n",
       "      <td>blogs\\1011289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Hey!!! Tonight kids, I saw Harry Potter and th...</td>\n",
       "      <td>blogs\\1011311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>RealEstate</td>\n",
       "      <td>male</td>\n",
       "      <td>Hey you clowns and kids with Down's. Sorry I h...</td>\n",
       "      <td>blogs\\1013637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>aint no such things as halfway crooks.   this ...</td>\n",
       "      <td>blogs\\1015252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Technology</td>\n",
       "      <td>male</td>\n",
       "      <td>well, the retreat was a success, i think. at l...</td>\n",
       "      <td>blogs\\1015556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>male</td>\n",
       "      <td>In his  urlLink February 14, 2003 post  Ray Ma...</td>\n",
       "      <td>blogs\\1016560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>male</td>\n",
       "      <td>as astute followers of the  urlLink Assistant ...</td>\n",
       "      <td>blogs\\1016738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>Leo</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>female</td>\n",
       "      <td>You love me... I have you here by my side... O...</td>\n",
       "      <td>blogs\\1016787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>RealEstate</td>\n",
       "      <td>female</td>\n",
       "      <td>Dear Susan,     You are a fat Wombat... I hate...</td>\n",
       "      <td>blogs\\1019224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>yay! it changed! :) i think i get it now. you ...</td>\n",
       "      <td>blogs\\1019622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>Yes i survived not eating for 24 hours, I am g...</td>\n",
       "      <td>blogs\\1019710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>IN THE SPACESHIP, THE SILVER SPACESHIP�  I fou...</td>\n",
       "      <td>blogs\\1021779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>Aaaaaahhhh....... mornings! know that it is st...</td>\n",
       "      <td>blogs\\1022037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>All right, I officially live in THE nut house....</td>\n",
       "      <td>blogs\\1022086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>wait...now dat i think about it...dat GBC thin...</td>\n",
       "      <td>blogs\\1024234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Lily's Lullaby- Part SIX  It was cold that eve...</td>\n",
       "      <td>blogs\\1025783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Education</td>\n",
       "      <td>female</td>\n",
       "      <td>I've posted a new test on my AIM profile...you...</td>\n",
       "      <td>blogs\\1026164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Hello hello hello again. It's lovely to speak ...</td>\n",
       "      <td>blogs\\1026443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>OK here goes nothing, I'm starting over. My ol...</td>\n",
       "      <td>blogs\\1028027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Education</td>\n",
       "      <td>male</td>\n",
       "      <td>Wednesday of this week, my wife Elizabeth and ...</td>\n",
       "      <td>blogs\\1028257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>When I think of nursery school, I think of the...</td>\n",
       "      <td>blogs\\2568090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>well i wrote this long elaborate entry this mo...</td>\n",
       "      <td>blogs\\2569244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Hey...right now i'm babysitting for my cousins...</td>\n",
       "      <td>blogs\\2571579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>Education</td>\n",
       "      <td>male</td>\n",
       "      <td>Ang mga baliw may pagamutan......ang mga tanga...</td>\n",
       "      <td>blogs\\2572049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>male</td>\n",
       "      <td>urlLink     Did You heard About The G-mail ......</td>\n",
       "      <td>blogs\\2572215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>my blog duznt work...=S...wen i click to see i...</td>\n",
       "      <td>blogs\\2572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>0</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>Technology</td>\n",
       "      <td>female</td>\n",
       "      <td>Breakfast...Raisin bran w/ l% fat milk  Lunch....</td>\n",
       "      <td>blogs\\2574489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>female</td>\n",
       "      <td>tired...the whole day in school was spend in m...</td>\n",
       "      <td>blogs\\2574746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>0</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>Arts</td>\n",
       "      <td>female</td>\n",
       "      <td>the gnomes are at it again.  the reckless crea...</td>\n",
       "      <td>blogs\\2574778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>It's New Years Eve and I'm full of empty promi...</td>\n",
       "      <td>blogs\\2575612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>female</td>\n",
       "      <td>So this is my first ever blog and I am pretty ...</td>\n",
       "      <td>blogs\\2575640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>\"An orange on the table, your dress on the rug...</td>\n",
       "      <td>blogs\\2575852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Education</td>\n",
       "      <td>male</td>\n",
       "      <td>In her latest entry,  urlLink sassy  talks abo...</td>\n",
       "      <td>blogs\\2576386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Here it is.. the last day of 2003.   Where the...</td>\n",
       "      <td>blogs\\2576645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Internet</td>\n",
       "      <td>female</td>\n",
       "      <td>I am not impress when I said I want a XDA O2 a...</td>\n",
       "      <td>blogs\\2576804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>i was so bored at work, i imed Steve and waite...</td>\n",
       "      <td>blogs\\2577859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>i had to lyk redo my blog? had one in diarylan...</td>\n",
       "      <td>blogs\\2578102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>0</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>today the last day of the year and everything ...</td>\n",
       "      <td>blogs\\2579349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>Arts</td>\n",
       "      <td>female</td>\n",
       "      <td>The Official Bon Jovi Site -  urlLink http://w...</td>\n",
       "      <td>blogs\\2579472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Technology</td>\n",
       "      <td>male</td>\n",
       "      <td>Bheegi Hui Koi - Chameli Mere Khwabon mein jo ...</td>\n",
       "      <td>blogs\\2580350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Oh, I'm so relieved. And happy. My birthday wa...</td>\n",
       "      <td>blogs\\2580846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>I wrote these yesterday for work but thought y...</td>\n",
       "      <td>blogs\\2581156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>There's  Gary .  he drinks just regular coffee...</td>\n",
       "      <td>blogs\\2581469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>female</td>\n",
       "      <td>and so it begins...  How exciting!  Very first...</td>\n",
       "      <td>blogs\\2581616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>\"Don't look at me, that's misleading. If you w...</td>\n",
       "      <td>blogs\\2581876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>0</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>Arts</td>\n",
       "      <td>male</td>\n",
       "      <td>Until now, I haven't been able to think.  I si...</td>\n",
       "      <td>blogs\\2582199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>0</td>\n",
       "      <td>Leo</td>\n",
       "      <td>Architecture</td>\n",
       "      <td>female</td>\n",
       "      <td>well. im trying out a new blog.. so u could ch...</td>\n",
       "      <td>blogs\\2583594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>female</td>\n",
       "      <td>urlLink    Don't ask. urlLink    What are we l...</td>\n",
       "      <td>blogs\\258556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Technology</td>\n",
       "      <td>female</td>\n",
       "      <td>First day on this new experiment of mine. I'm ...</td>\n",
       "      <td>blogs\\2585594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>urlLink    So I like the environment, give me ...</td>\n",
       "      <td>blogs\\2585707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age constellation                fields  gender  \\\n",
       "0       0           Leo                indUnk  female   \n",
       "1       0         Libra               Student  female   \n",
       "2       0     Capricorn                  Arts    male   \n",
       "3       0        Cancer                  Arts  female   \n",
       "4       0   Sagittarius           Engineering    male   \n",
       "5       0         Libra              Religion    male   \n",
       "6       0         Aries          Architecture  female   \n",
       "7       0        Pisces               Student  female   \n",
       "8       0        Cancer                indUnk    male   \n",
       "9       0         Virgo            Technology  female   \n",
       "10      0         Libra                indUnk  female   \n",
       "11      0       Scorpio                indUnk  female   \n",
       "12      0         Virgo            RealEstate    male   \n",
       "13      0        Pisces                indUnk  female   \n",
       "14      0         Virgo            Technology    male   \n",
       "15      0   Sagittarius            Publishing    male   \n",
       "16      0         Libra            Publishing    male   \n",
       "17      0           Leo  Communications-Media  female   \n",
       "18      0         Libra            RealEstate  female   \n",
       "19      0      Aquarius                indUnk  female   \n",
       "20      0        Pisces               Student    male   \n",
       "21      0       Scorpio                indUnk  female   \n",
       "22      0        Cancer                indUnk    male   \n",
       "23      0        Cancer               Student  female   \n",
       "24      0         Libra                indUnk  female   \n",
       "25      0        Gemini               Student  female   \n",
       "26      0         Aries             Education  female   \n",
       "27      0       Scorpio               Student  female   \n",
       "28      0         Libra                indUnk  female   \n",
       "29      0         Aries             Education    male   \n",
       "...   ...           ...                   ...     ...   \n",
       "1902    0       Scorpio                indUnk  female   \n",
       "1903    0         Virgo                indUnk  female   \n",
       "1904    0         Libra               Student  female   \n",
       "1905    0     Capricorn             Education    male   \n",
       "1906    0         Virgo  Communications-Media    male   \n",
       "1907    0   Sagittarius                indUnk  female   \n",
       "1908    0      Aquarius            Technology  female   \n",
       "1909    0        Pisces            Non-Profit  female   \n",
       "1910    0        Taurus                  Arts  female   \n",
       "1911    0        Gemini               Student    male   \n",
       "1912    0   Sagittarius             Marketing  female   \n",
       "1913    0        Gemini                indUnk  female   \n",
       "1914    0         Virgo             Education    male   \n",
       "1915    0        Cancer               Student  female   \n",
       "1916    0       Scorpio              Internet  female   \n",
       "1917    0   Sagittarius                indUnk  female   \n",
       "1918    0     Capricorn               Student  female   \n",
       "1919    0      Aquarius                indUnk    male   \n",
       "1920    0     Capricorn                  Arts  female   \n",
       "1921    0       Scorpio            Technology    male   \n",
       "1922    0        Taurus                indUnk  female   \n",
       "1923    0     Capricorn                indUnk  female   \n",
       "1924    0        Pisces               Student  female   \n",
       "1925    0   Sagittarius            Non-Profit  female   \n",
       "1926    0   Sagittarius                indUnk  female   \n",
       "1927    0        Taurus                  Arts    male   \n",
       "1928    0           Leo          Architecture  female   \n",
       "1929    0         Libra  Communications-Media  female   \n",
       "1930    0   Sagittarius            Technology  female   \n",
       "1931    0         Libra                indUnk    male   \n",
       "\n",
       "                                                   post        user_id  \n",
       "0     Well, everyone got up and going this morning. ...  blogs\\1000331  \n",
       "1     Yeah, sorry for not writing for a whole there,...  blogs\\1000866  \n",
       "2     cupid,please hear my cry, cupid, please let yo...  blogs\\1004904  \n",
       "3     and did i mention that i no longer have to dea...  blogs\\1005076  \n",
       "4     B-Logs: The Business Blogs Paradox    urlLink ...  blogs\\1005545  \n",
       "5     1/03 DrKioni.com Awarded ByRegion.net Healers ...  blogs\\1007188  \n",
       "6     Friday    My dear wife was walking on her grad...   blogs\\100812  \n",
       "7     Sorry, but I gotta..I couldn't remember the wo...  blogs\\1008329  \n",
       "8     Planning the Marathon   I checked Active.com, ...  blogs\\1009572  \n",
       "9     The astute among you will note that this run i...  blogs\\1011153  \n",
       "10    MSN conversation: 11.17am   Iggbalbollywall  (...  blogs\\1011289  \n",
       "11    Hey!!! Tonight kids, I saw Harry Potter and th...  blogs\\1011311  \n",
       "12    Hey you clowns and kids with Down's. Sorry I h...  blogs\\1013637  \n",
       "13    aint no such things as halfway crooks.   this ...  blogs\\1015252  \n",
       "14    well, the retreat was a success, i think. at l...  blogs\\1015556  \n",
       "15    In his  urlLink February 14, 2003 post  Ray Ma...  blogs\\1016560  \n",
       "16    as astute followers of the  urlLink Assistant ...  blogs\\1016738  \n",
       "17    You love me... I have you here by my side... O...  blogs\\1016787  \n",
       "18    Dear Susan,     You are a fat Wombat... I hate...  blogs\\1019224  \n",
       "19    yay! it changed! :) i think i get it now. you ...  blogs\\1019622  \n",
       "20    Yes i survived not eating for 24 hours, I am g...  blogs\\1019710  \n",
       "21    IN THE SPACESHIP, THE SILVER SPACESHIP�  I fou...  blogs\\1021779  \n",
       "22    Aaaaaahhhh....... mornings! know that it is st...  blogs\\1022037  \n",
       "23    All right, I officially live in THE nut house....  blogs\\1022086  \n",
       "24    wait...now dat i think about it...dat GBC thin...  blogs\\1024234  \n",
       "25    Lily's Lullaby- Part SIX  It was cold that eve...  blogs\\1025783  \n",
       "26    I've posted a new test on my AIM profile...you...  blogs\\1026164  \n",
       "27    Hello hello hello again. It's lovely to speak ...  blogs\\1026443  \n",
       "28    OK here goes nothing, I'm starting over. My ol...  blogs\\1028027  \n",
       "29    Wednesday of this week, my wife Elizabeth and ...  blogs\\1028257  \n",
       "...                                                 ...            ...  \n",
       "1902  When I think of nursery school, I think of the...  blogs\\2568090  \n",
       "1903  well i wrote this long elaborate entry this mo...  blogs\\2569244  \n",
       "1904  Hey...right now i'm babysitting for my cousins...  blogs\\2571579  \n",
       "1905  Ang mga baliw may pagamutan......ang mga tanga...  blogs\\2572049  \n",
       "1906  urlLink     Did You heard About The G-mail ......  blogs\\2572215  \n",
       "1907  my blog duznt work...=S...wen i click to see i...  blogs\\2572900  \n",
       "1908  Breakfast...Raisin bran w/ l% fat milk  Lunch....  blogs\\2574489  \n",
       "1909  tired...the whole day in school was spend in m...  blogs\\2574746  \n",
       "1910  the gnomes are at it again.  the reckless crea...  blogs\\2574778  \n",
       "1911  It's New Years Eve and I'm full of empty promi...  blogs\\2575612  \n",
       "1912  So this is my first ever blog and I am pretty ...  blogs\\2575640  \n",
       "1913  \"An orange on the table, your dress on the rug...  blogs\\2575852  \n",
       "1914  In her latest entry,  urlLink sassy  talks abo...  blogs\\2576386  \n",
       "1915  Here it is.. the last day of 2003.   Where the...  blogs\\2576645  \n",
       "1916  I am not impress when I said I want a XDA O2 a...  blogs\\2576804  \n",
       "1917  i was so bored at work, i imed Steve and waite...  blogs\\2577859  \n",
       "1918  i had to lyk redo my blog? had one in diarylan...  blogs\\2578102  \n",
       "1919  today the last day of the year and everything ...  blogs\\2579349  \n",
       "1920  The Official Bon Jovi Site -  urlLink http://w...  blogs\\2579472  \n",
       "1921  Bheegi Hui Koi - Chameli Mere Khwabon mein jo ...  blogs\\2580350  \n",
       "1922  Oh, I'm so relieved. And happy. My birthday wa...  blogs\\2580846  \n",
       "1923  I wrote these yesterday for work but thought y...  blogs\\2581156  \n",
       "1924  There's  Gary .  he drinks just regular coffee...  blogs\\2581469  \n",
       "1925  and so it begins...  How exciting!  Very first...  blogs\\2581616  \n",
       "1926  \"Don't look at me, that's misleading. If you w...  blogs\\2581876  \n",
       "1927  Until now, I haven't been able to think.  I si...  blogs\\2582199  \n",
       "1928  well. im trying out a new blog.. so u could ch...  blogs\\2583594  \n",
       "1929  urlLink    Don't ask. urlLink    What are we l...   blogs\\258556  \n",
       "1930  First day on this new experiment of mine. I'm ...  blogs\\2585594  \n",
       "1931  urlLink    So I like the environment, give me ...  blogs\\2585707  \n",
       "\n",
       "[1932 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "data = load_data(DATA_FILE_PATH_PATTERN, ratio=0.1)\n",
    "# Even with 16GB RAM, it was not enough to hold whole data,\n",
    "# so please use small data, but more than 10% of whole data.\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 11\n",
    "# Create Train, CV, Test\n",
    "def split_dataset(data, ratio=(.6, .2, .2), shuffle=False):\n",
    "    if sum(ratio) != 1.:\n",
    "        print \"Invalid data split ratio\", ratio\n",
    "        return\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    Add the condition whether shuffle the data or not.\n",
    "    \n",
    "    And then, split the data into train, validation, test set\n",
    "    \n",
    "    with given ratio.\n",
    "    \n",
    "    y values are from age column,\n",
    "    \n",
    "    and x values are from post column.\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    x = data['post'].values\n",
    "    y = data['age'].values\n",
    "    total_num = len(data)\n",
    "    \n",
    "    if(shuffle) :\n",
    "        #random\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        #to do random, use tempList murge x,y\n",
    "        tempList = [[x[0], y[0]]]\n",
    "        \n",
    "        for i in range(1,len(x)):\n",
    "            tempList.append([x[i],y[i]]) \n",
    "    \n",
    "        np.random.shuffle(tempList)\n",
    "        \n",
    "        #now we should seperate x, y\n",
    "        x = []\n",
    "        y = []\n",
    "        for tempData in tempList:\n",
    "            x.append(tempData[0])\n",
    "            y.append(tempData[1])\n",
    "    \n",
    "    #do seperate train, cv, test\n",
    "    x_train = x[0:total_num*6/10]\n",
    "    y_train = y[0:total_num*6/10]\n",
    "    x_cv = x[total_num*6/10:total_num*8/10]\n",
    "    y_cv = y[total_num*6/10:total_num*8/10]\n",
    "    x_test = x[total_num*8/10:total_num]\n",
    "    y_test = y[total_num*8/10:total_num]\n",
    "    \n",
    "    return (x_train, x_cv, x_test, y_train, y_cv, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  1159\n",
      "CV set    :  386\n",
      "Test set :  387\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_cv, x_test, y_train, y_cv, y_test) = split_dataset(df)\n",
    "print \"Train set: \" , len(x_train)\n",
    "print \"CV set    : \", len(x_cv)\n",
    "print \"Test set : \", len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Problem12.\n",
    "\n",
    "def get_bow_matrix(data, word_index=None, tf_mode=False):\n",
    "    # For Training set (with creating Word index)\n",
    "    if word_index == None:\n",
    "        return __get_bow_matrix_trains(data, tf_mode=tf_mode)\n",
    "    # for CV or Test set\n",
    "    else:\n",
    "        return __get_bow_matrix(data, word_index=word_index, tf_mode=tf_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_bow_matrix_trains(train_set, tf_mode):\n",
    "    bows = list()\n",
    "    \n",
    "    bows = [\n",
    "        __get_bow_dict(d, tf_mode=tf_mode)\n",
    "        for d in train_set\n",
    "    ]\n",
    "                \n",
    "    # Create word length dictionary\n",
    "    word_len = dict()\n",
    "    for bow in bows:\n",
    "        #bow of each post\n",
    "        for word in bow.keys():\n",
    "            if word not in word_len:\n",
    "                word_len[word] = 1\n",
    "            else:\n",
    "                word_len[word] += 1\n",
    "                \n",
    "    final_word_list = __get_word_list_by_cf(word_len,5000)\n",
    "    \n",
    "    # bow dict -> matrix\n",
    "    if tf_mode:\n",
    "        bow_matrix = np.zeros((len(bows), len(final_word_list)), dtype=np.uint16)\n",
    "    else:\n",
    "        bow_matrix = np.zeros((len(bows), len(final_word_list)), dtype=np.bool)\n",
    "    \n",
    "    for bow_idx in range(len(bows)):\n",
    "        bow = bows[bow_idx]\n",
    "        for w in bow:\n",
    "            if w in final_word_list:\n",
    "                bow_matrix[bow_idx, final_word_list.index(w)] = bow[w]\n",
    "            \n",
    "    return bow_matrix, final_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Training set.\n",
    "def __get_bow_dict(text, word_index=None, tf_mode = False):\n",
    "    bow = dict()\n",
    "\n",
    "    for word in wordpunct_tokenize(text):\n",
    "        # if word is in stopword_set\n",
    "        # only accept fully alphabet\n",
    "        word = word.lower()\n",
    "        if word in stopword_set or not word.isalpha():\n",
    "            continue      \n",
    "            \n",
    "        stem = stemmer.stem(word)\n",
    "        if word_index is not None:\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "                \n",
    "        \"\"\"\n",
    "        Create BOW dict\n",
    "        \"\"\"\n",
    "        # boolean mode (true or false)\n",
    "        if not tf_mode:\n",
    "            bow[stem] = 1\n",
    "        # Term Frequency Mode (bow[stem] = stemmed word occurance)\n",
    "        else:\n",
    "            if stem not in bow:\n",
    "                bow[stem] = 0\n",
    "            bow[stem] = bow[stem] + 1\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because the word list is too big, reduce word list, by collection frequency.\n",
    "def __get_word_list_by_cf(word_len_dict, k = 1000):\n",
    "    \n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    From the word_len_dict,\n",
    "    \n",
    "    please make frequency top k word list\n",
    "    \n",
    "    and return the list\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    #after zip, sort the dictionary by values\n",
    "    sorted_dict = sorted(zip(word_len_dict.values(),word_len_dict.keys()))\n",
    "    length = len(sorted_dict)\n",
    "    \n",
    "    top_k_word_list = []\n",
    "    #store k datas.\n",
    "    for i in range(1,k):\n",
    "        top_k_word_list.append(sorted_dict[length-i][1])\n",
    "    \n",
    "    return top_k_word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_bow_matrix(dataset, word_index, tf_mode):\n",
    "    if tf_mode:\n",
    "        matrix = np.zeros((len(dataset), len(word_index)), dtype=np.uint16)\n",
    "    else:\n",
    "        matrix = np.zeros((len(dataset), len(word_index)), dtype=np.bool)\n",
    "    \n",
    "    d_idx = 0\n",
    "    for data in dataset:\n",
    "        bow = __get_bow_dict(data, word_index, tf_mode)\n",
    "        for w in bow.keys():\n",
    "            if w in word_index:\n",
    "                matrix[d_idx, word_index.index(w)] = bow[w]\n",
    "        d_idx += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-dcbed949761f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbow_matrix_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bow_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mend_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Elapsed Time: \"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[0mend_t\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstart_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-a18140db3005>\u001b[0m in \u001b[0;36mget_bow_matrix\u001b[1;34m(data, word_index, tf_mode)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# For Training set (with creating Word index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m__get_bow_matrix_trains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# for CV or Test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-14ed354e69e2>\u001b[0m in \u001b[0;36m__get_bow_matrix_trains\u001b[1;34m(train_set, tf_mode)\u001b[0m\n\u001b[0;32m      4\u001b[0m     bows = [\n\u001b[0;32m      5\u001b[0m         \u001b[0m__get_bow_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     ]\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-d714204d4ceb>\u001b[0m in \u001b[0;36m__get_bow_dict\u001b[1;34m(text, word_index, tf_mode)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sec\\Anaconda2\\lib\\site-packages\\nltk\\stem\\lancaster.pyc\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseRules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrule_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doStemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintact_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__doStemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintact_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sec\\Anaconda2\\lib\\site-packages\\nltk\\stem\\lancaster.pyc\u001b[0m in \u001b[0;36m__doStemming\u001b[1;34m(self, word, intact_word)\u001b[0m\n\u001b[0;32m    253\u001b[0m                                 word = self.__applyRule(word,\n\u001b[0;32m    254\u001b[0m                                                         \u001b[0mremove_total\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                                                         append_string)\n\u001b[0m\u001b[0;32m    256\u001b[0m                                 \u001b[0mrule_was_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mcont_flag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sec\\Anaconda2\\lib\\site-packages\\nltk\\stem\\lancaster.pyc\u001b[0m in \u001b[0;36m__applyRule\u001b[1;34m(self, word, remove_total, append_string)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;31m# And add new letters to the end of the truncated word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mappend_string\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mappend_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################\n",
    "\n",
    "# not finish this process . so i can't go to the next step.\n",
    "# i don't know where is the problem .\n",
    "\n",
    "start_t = time.time()\n",
    "bow_matrix_train, word_index = get_bow_matrix(x_train, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)\n",
    "start_t = time.time()\n",
    "bow_matrix_cv = get_bow_matrix(x_cv, word_index, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)\n",
    "start_t = time.time()\n",
    "bow_matrix_test = get_bow_matrix(x_test, word_index, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Problem 13\n",
    "\n",
    "svc_model = SVC()\n",
    "\n",
    "###########################################################\n",
    "########## fit the model by svc model              ########\n",
    "########## and predict the value of validation set ########\n",
    "########## set the parameter,                      ########\n",
    "########## then predict the test set               ########\n",
    "###########################################################\n",
    "svc_model.fit(bow_matrix_train, y_train)\n",
    "pred_cv = svc_model.predict(bow_matrix_cv)\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv)\n",
    "print prf[2]\n",
    "\n",
    "\n",
    "'''\n",
    "###### because of previous step, i can't check this step.\n",
    "###### but i just Predict modify 'above code' to 'below code'\n",
    "\n",
    "max = 0;\n",
    "for degree in range(5):\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(bow_matrix_train, y_train)\n",
    "    pred_cv = svc_model.predict(bow_matrix_cv)\n",
    "    prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv)\n",
    "    if prf[2] > max:\n",
    "        fin_svc_model = svc_model\n",
    "        max = prf[2]\n",
    "        \n",
    "svc_model = fin_svc_model # now we choose best model by cv datas.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the prediction result of test set\n",
    "pred_test = svc_model.predict(bow_matrix_test)\n",
    "prf_test = sklearn.metrics.precision_recall_fscore_support(y_test, pred_test)\n",
    "print prf_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
